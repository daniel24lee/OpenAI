// https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models

import { providers_info } from "#root/src/api/providers.js";

// GET /api/tags
export function v1_models_endpoint(req, res) {
    const models = [];

    for (const key of Object.keys(providers_info)) {
        if (!key) continue;
        models.push({
            name: key,
            model: key,
            modified_at: "2023-10-01T00:00:00Z",
            size: 815319791,
            "digest": "8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc",
            "details": {
                "parent_model": "",
                "format": "gguf",
                "family": "llama",
                "families": [
                    "llama"
                ],
                "parameter_size": "8.0B",
                "quantization_level": "Q4_0"
            }
        });
    }

    res.json({ "models": models });
}

// POST /api/show
export function v1_show_endpoint(req, res) {
    // console.log(req.body);
    const model = req.body.model;
    if (!model || !providers_info[model]) {
        return res.status(400).json({ error: "Model not found" });
    }

    res.json(v1_show_response);
}


// --- Constants
const v1_show_response = {
    "modelfile": "# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this one, replace the FROM line with:\n# FROM llava:latest\n\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\nTEMPLATE \"\"\"{{ .System }}\nUSER: {{ .Prompt }}\nASSISTANT: \"\"\"\nPARAMETER num_ctx 100000\nPARAMETER stop \"\u003c/s\u003e\"\nPARAMETER stop \"USER:\"\nPARAMETER stop \"ASSISTANT:\"",
    "parameters": "num_keep                       24\nstop                           \"<|start_header_id|>\"\nstop                           \"<|end_header_id|>\"\nstop                           \"<|eot_id|>\"",
    "template": "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
    "details": {
        "parent_model": "",
        "format": "gguf",
        "family": "llama",
        "families": [
            "llama"
        ],
        "parameter_size": "8.0B",
        "quantization_level": "Q4_0"
    },
    "model_info": {
        "general.architecture": "llama",
        "general.file_type": 2,
        "llama.context_length": 5000000, // Set to very high value to avoid context length issues
    },
    "capabilities": ["completion", "vision"]
};